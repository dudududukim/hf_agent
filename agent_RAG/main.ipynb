{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa267304",
   "metadata": {},
   "source": [
    "# agent_RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cab52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimduhyeon/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/kimduhyeon/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Import necessary libraries\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212e91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def empty_tool() -> str:\n",
    "    \"\"\"This is an empty tool that does nothing.\"\"\"\n",
    "    return \"Empty tool executed\"\n",
    "\n",
    "empty_tool_instance = FunctionTool.from_defaults(\n",
    "    fn=empty_tool,\n",
    "    name=\"empty_tool\",\n",
    "    description=\"An empty tool that does nothing but returns a simple message\"\n",
    ")\n",
    "\n",
    "def empty_tool_with_params(param: str) -> str:\n",
    "    \"\"\"This is an empty tool that accepts parameters but does nothing.\"\"\"\n",
    "    return f\"Empty tool executed with param: {param}\"\n",
    "\n",
    "empty_tool_with_params_instance = FunctionTool.from_defaults(\n",
    "    fn=empty_tool_with_params,\n",
    "    name=\"empty_tool_with_params\",\n",
    "    description=\"An empty tool that accepts parameters but performs no action\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf29f5a",
   "metadata": {},
   "source": [
    "### with provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa41de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face model\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\", provider=\"auto\")\n",
    "\n",
    "# Create Alfred with all the tools\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [empty_tool, empty_tool_with_params],\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b5b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ© Alfred's Response:\n",
      "Lady Ada Lovelace, born Augusta Ada King-Noel on December 10, 1815, was an English mathematician and writer. She is widely regarded as the first computer programmer. Her father was the renowned poet Lord Byron, and her mother, Anne Isabella Milbanke, encouraged Ada to focus on mathematics and logic to counteract what she saw as Byron's poetic and somewhat unstable nature. Ada received tutoring from some of the best mathematicians of her time, including Augustus De Morgan and Mary Somerville. In 1833, she met Charles Babbage, who was working on his Analytical Engine, a mechanical general-purpose computer. Lovelace translated an article by Italian mathematician Luigi Menabrea about the engine and added her own notes, which were more extensive than the original article. These notes included an algorithm intended to be processed by the machine, making her the first person to write instructions for a computer program. Lovelace passed away on November 27, 1852, at the age of 36.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about Lady Ada Lovelace. What's her background?\"\n",
    "response = await alfred.run(query)\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response.response.blocks[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73d480",
   "metadata": {},
   "source": [
    "### withou provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96174a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Hugging Face model\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# Create Alfred with all the tools\n",
    "alfred = AgentWorkflow.from_tools_or_functions(\n",
    "    [empty_tool],\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3950a81e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientResponseError",
     "evalue": "404, message='Not Found', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientResponseError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mTell me about Lady Ada Lovelace. What\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms her background?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m alfred.run(query)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸŽ© Alfred\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Response:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.response.blocks[\u001b[32m0\u001b[39m].text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/workflows/workflow.py:439\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    437\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    443\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/workflows/context/context.py:822\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, verbose, run_id, worker_id, resource_manager)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28mself\u001b[39m.write_event_to_stream(\n\u001b[32m    814\u001b[39m     StepStateChanged(\n\u001b[32m    815\u001b[39m         name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    819\u001b[39m     )\n\u001b[32m    820\u001b[39m )\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    823\u001b[39m     kwargs.clear()\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py:410\u001b[39m, in \u001b[36mAgentWorkflow.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    407\u001b[39m user_msg_str = \u001b[38;5;28;01mawait\u001b[39;00m ctx.store.get(\u001b[33m\"\u001b[39m\u001b[33muser_msg_str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    408\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(ev.current_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m agent.take_step(\n\u001b[32m    411\u001b[39m     ctx,\n\u001b[32m    412\u001b[39m     ev.input,\n\u001b[32m    413\u001b[39m     tools,\n\u001b[32m    414\u001b[39m     memory,\n\u001b[32m    415\u001b[39m )\n\u001b[32m    417\u001b[39m ctx.write_event_to_stream(agent_output)\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m agent_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/react_agent.py:145\u001b[39m, in \u001b[36mReActAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Initial LLM call\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.streaming:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     last_chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_streaming_response(ctx, input_chat)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    147\u001b[39m     last_chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_response(input_chat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/react_agent.py:92\u001b[39m, in \u001b[36mReActAgent._get_streaming_response\u001b[39m\u001b[34m(self, ctx, current_llm_input)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# last_chat_response will be used later, after the loop.\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# We initialize it so it's valid even when 'response' is empty\u001b[39;00m\n\u001b[32m     91\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m     93\u001b[39m     raw = (\n\u001b[32m     94\u001b[39m         last_chat_response.raw.model_dump()\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response.raw, BaseModel)\n\u001b[32m     96\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response.raw\n\u001b[32m     97\u001b[39m     )\n\u001b[32m     98\u001b[39m     ctx.write_event_to_stream(\n\u001b[32m     99\u001b[39m         AgentStream(\n\u001b[32m    100\u001b[39m             delta=last_chat_response.delta \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m         )\n\u001b[32m    108\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py:462\u001b[39m, in \u001b[36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    460\u001b[39m tool_call_strs = []\n\u001b[32m    461\u001b[39m cur_index = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat_completion(\n\u001b[32m    463\u001b[39m     messages=\u001b[38;5;28mself\u001b[39m._to_huggingface_messages(messages),\n\u001b[32m    464\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    465\u001b[39m     **model_kwargs,\n\u001b[32m    466\u001b[39m ):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.choices[\u001b[32m0\u001b[39m].finish_reason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py:959\u001b[39m, in \u001b[36mAsyncInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    931\u001b[39m parameters = {\n\u001b[32m    932\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    933\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    950\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    951\u001b[39m }\n\u001b[32m    952\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    953\u001b[39m     inputs=messages,\n\u001b[32m    954\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    957\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    958\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_post(request_parameters, stream=stream)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    962\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _async_stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py:285\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    283\u001b[39m     error.response_error_payload = response_error_payload\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m session.close()\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m session.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py:271\u001b[39m, in \u001b[36mAsyncInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    270\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _async_yield_from(session, response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hf_ai_agent_course/.venv/lib/python3.12/site-packages/aiohttp/client_reqrep.py:638\u001b[39m, in \u001b[36mClientResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_context:\n\u001b[32m    636\u001b[39m     \u001b[38;5;28mself\u001b[39m.release()\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m    639\u001b[39m     \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m    641\u001b[39m     status=\u001b[38;5;28mself\u001b[39m.status,\n\u001b[32m    642\u001b[39m     message=\u001b[38;5;28mself\u001b[39m.reason,\n\u001b[32m    643\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.headers,\n\u001b[32m    644\u001b[39m )\n",
      "\u001b[31mClientResponseError\u001b[39m: 404, message='Not Found', url='https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions'"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about Lady Ada Lovelace. What's her background?\"\n",
    "response = await alfred.run(query)\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response.response.blocks[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b82fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_agent",
   "language": "python",
   "name": "hf_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
